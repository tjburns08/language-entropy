# language-entropy

This project came about when I was working on my German language learning. German has some very long words, and often times seems very inefficient at conveying information. But on the other hand, the same building-block word structure allows for some information to be conveyed very nicely in German as opposed to English. So which language is more efficient? This was my initial attempt to answer that question by looking at information content based on letter usage frequency across the English alphabet versus the German alphabet (which contains a few extra omlautted letters and a letter that is essentially "ss.")

This project is currently on hold due to both my current obligations as a computational biologist, and my intent to restart this as an n-grams study, which is a bit more complicated. Nonetheless, I leave it here for the reader to chew on. 
